{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332db16-6a14-4d13-8b68-81a66b412a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, RepeatVector, TimeDistributed, Bidirectional, Lambda, Masking, Multiply\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, LayerNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Reshape\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from keras.constraints import Constraint\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" \n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.random.set_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731023f-1565-4ec3-8c5f-24a4f7a281bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the different types of data\n",
    "\n",
    "path = '/home/chengli/dataset/df_varsAll_cleaned_withHBA1c_withBMI_pred_imp_v6.csv'\n",
    "df_EHRs = pd.read_csv(path, index_col=0)\n",
    "\n",
    "#variables = ['HBA1C']\n",
    "variables =  ['TG', 'CREAT', 'CAC', 'COLHDL', 'COLTOT', 'COLLDL', 'HBA1C', \n",
    "             'EK201', 'EK202', 'TT103']\n",
    "variables_pred = [v+'_pred_death_missings' for v in variables]\n",
    "variables_pred2 = [v+'_pred' for v in variables]\n",
    "col = variables_pred2\n",
    "\n",
    "def replace_na(x):\n",
    "    n_na = x.isna().sum()\n",
    "    if n_na<len(variables_pred) + 2:\n",
    "        for i,v in enumerate(variables_pred):\n",
    "            x[v] = x[variables_pred[i]]\n",
    "    return x\n",
    "\n",
    "df_EHRs.loc[df_EHRs.ttd>0,'ttd'] = 0\n",
    "df_EHRs['ttd'] = df_EHRs['ttd'].fillna(-6)\n",
    "df_EHRs['ttd'] = df_EHRs['ttd']*-1\n",
    "df_EHRs = df_EHRs.drop(columns = ['age_death'])\n",
    "df_EHRs = df_EHRs.apply(replace_na, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a989a-e81e-4cda-87c6-35ffb9a69a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EHRs[variables_pred2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5959d-484d-4650-8cdb-c376987259eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '~/send_chengli/kernel_matrix_allVars_cleaned_v7.pkl'\n",
    "K_matrix = pd.read_pickle(file)\n",
    "idps_list = K_matrix.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973b73f-ce53-49bd-94ad-bd205335aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "df_EHRs[col] = (df_EHRs[col]-df_EHRs[col].min())/(df_EHRs[col].max()-df_EHRs[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8a5e7-cf7e-406d-b513-ef8eed87a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pre = col + ['months_from_diag']\n",
    "df_EHRs.sex = df_EHRs.sex.replace(to_replace = ['H', 'D'], value=[1,0])\n",
    "def get_real_df(idp):\n",
    "    df = df_EHRs.loc[df_EHRs.idp==idp, col_pre]\n",
    "    df = df.sort_values(by='months_from_diag')\n",
    "    df = df.loc[:, col_pre]\n",
    "    df.loc[df_EHRs.isna().any(axis=1)] = -0.1\n",
    "    df.drop('months_from_diag', axis=1)\n",
    "    return np.array(df, dtype=np.float32)\n",
    "n_cpu = 10\n",
    "with mp.Pool(n_cpu) as pool:\n",
    "    df_ehrs_real = pool.map(get_real_df, idps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11e833-ff47-4ddf-ac2a-2a9ee9a89ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df_ehrs_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a4c84-3213-45b0-bbc4-755d142c6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "     def __init__(self, clip_value):\n",
    "            self.clip_value = clip_value\n",
    "    \n",
    "    # clip model weights to hypercube\n",
    "     def __call__(self, weights):\n",
    "            return K.clip(weights, -self.clip_value, self.clip_value)\n",
    " \n",
    "     # get the config\n",
    "     def get_config(self):\n",
    "             return {'clip_value': self.clip_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216cee7-c650-404a-bac3-27fc23ed1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrices_loss(k_pred,k_true):\n",
    "    \n",
    "    k_pred = k_pred/tf.norm(k_pred)\n",
    "    k_true = k_true/tf.norm(k_true)\n",
    "    L = tf.norm(k_pred-k_true)\n",
    "    return L\n",
    "\n",
    "def vectors_loss(mask_value):\n",
    "    mask_value = K.variable(mask_value)\n",
    "    def masked_mse(y_true, y_pred):\n",
    "        # find out which timesteps in `y_true` do not contain mascked value\n",
    "        mask = K.not_equal(y_true, mask_value)\n",
    "        mask = K.cast(mask, K.floatx())\n",
    "\n",
    "        # multiply categorical_crossentropy with the mask\n",
    "        loss = (y_true-y_pred)*mask\n",
    "        loss = K.square(loss) \n",
    "\n",
    "        # take average w.r.t. the number of unmasked entries\n",
    "        return K.sum(loss) / K.sum(mask)\n",
    "    return masked_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f4712-aa8d-458d-b5d0-746b97295c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "     return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643292f-fefe-412a-b3ae-1087fba5b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the self-attention layer\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = self.add_weight(shape=[input_shape[-1], 1], initializer=\"glorot_uniform\", trainable=True, name=\"query\")\n",
    "        self.key = self.add_weight(shape=[input_shape[-1], 1], initializer=\"glorot_uniform\", trainable=True, name=\"key\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query_scores = tf.einsum('btk,kc->btc', inputs, self.query)\n",
    "        key_scores = tf.einsum('btk,kc->btc', inputs, self.key)\n",
    "\n",
    "        attention_scores = tf.nn.softmax(query_scores * key_scores, axis=1)\n",
    "\n",
    "        output = tf.einsum('btc,btk->bkc', attention_scores, inputs)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SelfAttentionLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1372f4d-11bd-4300-8f56-7842c568ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the critic\n",
    "def define_critic(disc_dim=200, inp_dim=10, time_step=10):\n",
    "    \n",
    "    # weight initialization\n",
    "    init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    # weight constraint\n",
    "    const = ClipConstraint(0.5)\n",
    "    # define the model\n",
    "    inp = Input(shape=(time_step, inp_dim,))\n",
    "    x = SelfAttentionLayer()(inp)\n",
    "    x = LSTM(disc_dim, return_sequences=True, kernel_initializer=init, kernel_constraint=const)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = LSTM(disc_dim, return_sequences=True, kernel_initializer=init, kernel_constraint=const)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    #x = LSTM(20, kernel_initializer=init, kernel_constraint=const)(x)\n",
    "    #x = LayerNormalization()(x)\n",
    "    x = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x, name='discriminator')\n",
    "    \n",
    "    # compile the model\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34a075-8eb8-4cf5-ba27-0da1de5dfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_decoder(vec_dim=100, latent_dim=200):\n",
    "    \n",
    "    decoder = Model(inputs=model_autoencoder.layers[7].input, outputs=model_autoencoder.outputs[0])\n",
    "    decoder.trainable = False\n",
    "  \n",
    "    inp = Input(shape=(vec_dim,))   \n",
    "    x = Dense(latent_dim)(inp)\n",
    "    x = Dense(latent_dim)(x)\n",
    "    x = decoder(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x, name='generator_decoder')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b7c1b-db6f-4b7e-899f-71f3b99f69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_model(verbose=False, vec_dim=100):\n",
    "    \n",
    "    discriminator = define_critic()\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    #discriminator.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    generator = gen_decoder()\n",
    "    \n",
    "    inp = Input(shape=(vec_dim,))\n",
    "    x = generator(inp)\n",
    "    x = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    # compile the model\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "    gan.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "        \n",
    "    if verbose:\n",
    "        generator.summary()\n",
    "        discriminator.summary()\n",
    "        gan.summary()\n",
    "    \n",
    "    return generator, discriminator, gan\n",
    "\n",
    "generator_con, discriminator_con, gan_con = con_model(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22de6d-9371-4e58-ad18-e247272b84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(BATCH_SIZE=128):   \n",
    "    return np.random.rand(BATCH_SIZE, 100).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b15d86-1931-4422-b1e3-5affc25b88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of discriminator updates per alternating training iteration\n",
    "DISC_UPDATES = 10 # COMM: have you tried to play a bit with these two parameters? what changes increasing the GEN steps?\n",
    "# number of generator updates per alternating traning iteration\n",
    "GEN_UPDATES = 1\n",
    "# define the save interval\n",
    "SAVE_INTERVAL = 20\n",
    "# define the batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "\n",
    "def run_training(gan,generator, discriminator, num_epochs=100, save_freq=10):\n",
    "     \n",
    "    half_batch = int(BATCH_SIZE/2)\n",
    "    df_real = np.array(df_ehrs_real, dtype=np.float32)\n",
    "    \n",
    "    # main training loop   \n",
    "        for iteration in range(200):\n",
    "            \n",
    "            c1_tmp, c2_tmp, g_tmp = list(), list(), list()\n",
    "            discriminator.trainable = True\n",
    "            # discriminator training loop\n",
    "            for _ in range(DISC_UPDATES):\n",
    "                \n",
    "                # select a random set of real EHRs\n",
    "                rand_int = np.random.randint(0, df_real.shape[0]-half_batch)\n",
    "                EHRs_real = df_real[rand_int:(rand_int+half_batch), :, :]\n",
    "                # generate a set of random noise vectors\n",
    "                noise = get_noise(BATCH_SIZE=half_batch)\n",
    "                # generate a set of fake EHRs\n",
    "                EHRs_fake = generator.predict(noise)\n",
    "                # training the discriminator on real EHRs with label -1\n",
    "                d_loss_real = discriminator.train_on_batch(EHRs_real, -np.ones([half_batch, 1], dtype=np.float32))\n",
    "                c1_tmp.append(d_loss_real)\n",
    "                # training the discriminator on fake EHRs with label 1\n",
    "                d_loss_fake = discriminator.train_on_batch(EHRs_fake, np.ones([half_batch, 1], dtype=np.float32))\n",
    "                c2_tmp.append(d_loss_fake)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # generator training loop\n",
    "            loss = 0\n",
    "            y = -np.ones([BATCH_SIZE, 1], dtype=np.float32)\n",
    "            for _ in range(GEN_UPDATES):\n",
    "                # generate a set of random noise vectors\n",
    "                noise = get_noise(BATCH_SIZE=BATCH_SIZE)\n",
    "                # train the generator on fake EHRs with label 1\n",
    "                loss += gan.train_on_batch(noise, y)\n",
    "            g_tmp.append(loss / GEN_UPDATES)  \n",
    "        \n",
    "        clear_output(True)\n",
    "               \n",
    "        # visualize the loss\n",
    "        print('Epoch', epoch)\n",
    "        c1_hist.append(np.mean(c1_tmp))\n",
    "        c2_hist.append(np.mean(c2_tmp))\n",
    "        g_hist.append(np.mean(g_tmp))\n",
    "        plt.plot(range(len(c1_hist)), c1_hist)\n",
    "        plt.plot(range(len(c2_hist)), c2_hist)\n",
    "        plt.plot(range(len(g_hist)), g_hist)\n",
    "        plt.legend(['disc_real_loss', 'disc_fake_loss', 'gen_loss'])\n",
    "        plt.show()\n",
    "        nosie = get_noise()\n",
    "        df = generator.predict(noise)\n",
    "        print(df[10,:,:])\n",
    "        \n",
    "    return generator, discriminator, gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41601fa0-de12-4f4d-bd2d-9d9cd1f9f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_EHRs_trained, discriminator_EHRs_trained, gan_EHRs_trained = run_training(gan_con,\n",
    "                                                                                    generator_con,\n",
    "                                                                                    discriminator_con,\n",
    "                                                                                    num_epochs=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
