{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 20:04:44.966861: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-08-25 20:04:44.966880: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jazz): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" \n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.random.set_seed(22)\n",
    "\n",
    "# Keras layers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, RepeatVector, TimeDistributed, Bidirectional, Lambda, Masking, Multiply\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/chengli/dataset/df_varsAll_cleaned_withHBA1c_withBMI_pred_imp_v6.csv'\n",
    "df_EHRs = pd.read_csv(path, index_col=0)\n",
    "\n",
    "file = '~/send_chengli/kernel_matrix_allVars_cleaned_v7.pkl'\n",
    "K_matrix = pd.read_pickle(file)\n",
    "idps_list = K_matrix.index\n",
    "\n",
    "#variables = ['HBA1C']\n",
    "variables = ['TG', 'CREAT', 'CAC', 'COLHDL', 'COLTOT', 'COLLDL', 'HBA1C', \n",
    "             'EK201', 'EK202', 'TT103']\n",
    "variables_pred = [v+'_pred_death_missings' for v in variables]\n",
    "variables_pred2 = [v+'_pred' for v in variables]\n",
    "\n",
    "generals = ['age','months_from_diag','sex','age_diag', 'ttd']\n",
    "diags = ['HTA','CI','NEFRPDM','RTP_DM','NEUROPT']\n",
    "\n",
    "col = variables_pred2\n",
    "\n",
    "\n",
    "def replace_na(x):\n",
    "    n_na = x.isna().sum()\n",
    "    if n_na<len(variables_pred) + 2:\n",
    "        for i,v in enumerate(variables_pred):\n",
    "            x[v] = x[variables_pred[i]]\n",
    "    return x\n",
    "\n",
    "df_EHRs.loc[df_EHRs.ttd>0,'ttd'] = 0\n",
    "df_EHRs['ttd'] = df_EHRs['ttd'].fillna(-6)\n",
    "df_EHRs['ttd'] = df_EHRs['ttd']*-1\n",
    "df_EHRs = df_EHRs.drop(columns = ['age_death'])\n",
    "df_EHRs = df_EHRs.apply(replace_na, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/enrico/Desktop/dm2_code/autoencoder/kernel_matrix_allVars_cleaned_v7.csv', K_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = variables_pred2 + ['age','months_from_diag','age_diag', 'ttd']\n",
    "df_EHRs.sex = df_EHRs.sex.replace(to_replace = ['H', 'D'], value=[1,0])\n",
    "df_EHRs[norm_cols] = (df_EHRs[norm_cols]-df_EHRs[norm_cols].min())/(df_EHRs[norm_cols].max()-df_EHRs[norm_cols].min())\n",
    "cols_tmp = [col for col in df_EHRs.columns if col not in ['idp', 'months_from_diag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all continues variables\n",
    "col_pre = col + ['months_from_diag']\n",
    "def get_real_df(idp):\n",
    "    df = df_EHRs.loc[df_EHRs.idp==idp, col_pre]\n",
    "    df = df.sort_values(by='months_from_diag')\n",
    "    df = df.loc[:, col]\n",
    "    df.loc[df_EHRs.isna().any(axis=1)] = -0.1\n",
    "    #df.drop('months_from_diag', axis=1)\n",
    "    return np.array(df, dtype=np.float32)\n",
    "n_cpu = 10\n",
    "with mp.Pool(n_cpu) as pool:\n",
    "    X_set = np.stack(list((pool.map(get_real_df, idps_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_set(idp):\n",
    "    df = df_anal.loc[df_anal.idp==idp,cols]\n",
    "    df = df.sort_values(by='months_from_diag')\n",
    "    df.loc[df_anal.isna().any(axis=1)] = -0.1\n",
    "    return np.array(df)\n",
    "\n",
    "n_cpu = 10\n",
    "with mp.Pool(n_cpu) as pool:\n",
    "    X_set = np.stack(list((pool.map(prepare_X_set,idps_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11028, 10, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Y_set(idp):\n",
    "    df = df_EHRs.loc[df_EHRs.idp==idp,col]\n",
    "    return np.array(df)\n",
    "\n",
    "Y_set = [X_set,K_matrix]\n",
    "#with mp.Pool(n_cpu) as pool:\n",
    " #   Y_set = np.stack(list((pool.map(prepare_Y_set,idps_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrices_loss(k_pred,k_true):\n",
    "    \n",
    "    k_pred = k_pred/tf.norm(k_pred)\n",
    "    k_true = k_true/tf.norm(k_true)\n",
    "    L = tf.norm(k_pred-k_true)\n",
    "    return L\n",
    "\n",
    "def vectors_loss(mask_value):\n",
    "    mask_value = K.variable(mask_value)\n",
    "    def masked_mse(y_true, y_pred):\n",
    "        # find out which timesteps in `y_true` do not contain mascked value\n",
    "        mask = K.not_equal(y_true, mask_value)\n",
    "        mask = K.cast(mask, K.floatx())\n",
    "\n",
    "        # multiply categorical_crossentropy with the mask\n",
    "        loss = (y_true-y_pred)*mask\n",
    "        loss = K.square(loss) \n",
    "\n",
    "        # take average w.r.t. the number of unmasked entries\n",
    "        return K.sum(loss) / K.sum(mask)\n",
    "    return masked_mse\n",
    "\n",
    "\n",
    "\n",
    "def create_autoencoder(enc_dim = 200,repr_dim = 200, dec_dim = 200, alpha_kernel=0.5, alpha_out=1, mask_value=-0.1):\n",
    "    \n",
    "    # define parameters\n",
    "    time_steps = pd.date_range(start='2013-01-31', end='2017-07-31', freq='6M')\n",
    "    n_time_steps = len(time_steps)\n",
    "    n_input = len(col)\n",
    "    n_output = len(col)\n",
    "    n_idps = len(idps_list)\n",
    "    \n",
    "    # encoder\n",
    "    #inputs = Input(batch_shape=(n_idps,n_time_steps,n_input))\n",
    "    inputs = Input(shape=(n_time_steps,n_input))\n",
    "    #mask = Masking(mask_value=-0.1)(inputs)\n",
    "    encoder_forward = LSTM(enc_dim, return_sequences=True)(inputs)\n",
    "    encoder_forward = LSTM(enc_dim)(encoder_forward)\n",
    "    encoder_back = LSTM(enc_dim, return_sequences=True, go_backwards=True)(inputs)\n",
    "    encoder_back = LSTM(enc_dim, go_backwards=True)(encoder_back)\n",
    "    encoder = Concatenate()([encoder_forward, encoder_back])\n",
    "    encoder = Dense(repr_dim)(encoder)\n",
    "    # kernel alligment\n",
    "    kernel_alligment = Lambda(lambda x: K.dot(x,K.transpose(x)), name='kernel_output')(encoder)\n",
    "    # decoder\n",
    "    decoder = RepeatVector(n_time_steps)(encoder)\n",
    "    decoder = LSTM(dec_dim, return_sequences=True)(decoder)\n",
    "    decoder = LSTM(dec_dim, return_sequences=True)(decoder)\n",
    "    decoder = TimeDistributed(Dense(n_output), name='decoder_output')(decoder)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[decoder,kernel_alligment])\n",
    "    masked_mse = vectors_loss(-0.1)\n",
    "    losses = {'kernel_output': matrices_loss, 'decoder_output': masked_mse}\n",
    "    lossesWeight = {'kernel_output': alpha_kernel, 'decoder_output':alpha_out}\n",
    "\n",
    "    model.compile(optimizer='adam', loss=losses, loss_weights= lossesWeight)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "alphas = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "alpha2 = ['00','01','02','03','04','05','06','07','08','09','10']  \n",
    "\n",
    "enc_dims = [100, 150, 200]\n",
    "dec_dims= [100, 150, 200]\n",
    "repr_dims = [100, 150, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "embs_done = glob.glob('/home/chengli/model/Autoencoder/embedded3/*.csv')\n",
    "hist = []\n",
    "for i,alpha in enumerate(alphas):\n",
    "    for enc_dim in enc_dims:\n",
    "        for dec_dim in dec_dims:\n",
    "            for repr_dim in repr_dims:\n",
    "                #chek if emb was already calculated\n",
    "                embs_name = '/home/chengli/model/Autoencoder/embedded3/embs_autoencoder_allVars_clean_kernelImp_alpha{}_enc{}_dec{}_repr{}.csv'.format(alpha2[i],enc_dim,dec_dim,repr_dim)\n",
    "                print('Working on variables: {}'.format(embs_name[100:-4]))\n",
    "                if embs_name not in embs_done:\n",
    "                    filename = '/home/chengli/model/Autoencoder/autoencoder_alpha{}_enc{}_dec{}_repr{}.h5'.format(alpha2[i],enc_dim,dec_dim,repr_dim)\n",
    "                    checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "                    # create model\n",
    "                    model = create_autoencoder(alpha_kernel=alpha, repr_dim=repr_dim, enc_dim=enc_dim, dec_dim=dec_dim)\n",
    "                    #model.summary()\n",
    "                    history = model.fit(X_set, Y_set, batch_size=len(X_set), epochs=25, verbose=0, callbacks=[checkpoint])\n",
    "                    train_loss = history.history['loss']\n",
    "                    hist.append(train_loss[-1])\n",
    "                    # get representation\n",
    "                    model_autoencoder = Model(inputs= model.inputs, outputs= model.layers[6].output)\n",
    "                    #model_autoencoder.summary()\n",
    "                    Y_pred = model_autoencoder.predict(X_set)\n",
    "                    representations = pd.DataFrame(Y_pred, index=idps_list)\n",
    "                    # save embs\n",
    "                    representations.to_csv(embs_name)\n",
    "                    K.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_autoencoder(enc_dim=200, dec_dim=150, repr_dim=200, alpha_kernel=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0042 - decoder_output_loss: 0.0042 - kernel_output_loss: 0.5536\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0043 - decoder_output_loss: 0.0043 - kernel_output_loss: 0.5530\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0042 - decoder_output_loss: 0.0042 - kernel_output_loss: 0.5526\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0040 - decoder_output_loss: 0.0040 - kernel_output_loss: 0.5520\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0040 - decoder_output_loss: 0.0040 - kernel_output_loss: 0.5516\n"
     ]
    }
   ],
   "source": [
    "#model = create_autoencoder(alpha_kernel=0.5, repr_dim=100, enc_dim=200, dec_dim=200)\n",
    "com_hist = model.fit(X_set, Y_set,  batch_size=len(X_set), epochs=50, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/chengli/model/Autoencoder/Autoencoder.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
